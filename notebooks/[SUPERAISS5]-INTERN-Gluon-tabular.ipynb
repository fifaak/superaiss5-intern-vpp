{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1rQd2IyNGs-XBNZE9Vximf1X6JFOuM9qL","timestamp":1755678192363}],"gpuType":"T4","authorship_tag":"ABX9TyOuLdphCrEtn9s9CRHoOfry"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Import Dataset"],"metadata":{"id":"gDjoDg42kMGv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y9ekDThljjj0"},"outputs":[],"source":["!gdown --folder https://drive.google.com/drive/folders/1dydbU9HlSIgGQBzYMLogDNI27uO6wga7?usp=drive_link"]},{"cell_type":"markdown","source":["## Load & Clean the Data"],"metadata":{"id":"ABStU3J8kQ9D"}},{"cell_type":"code","source":["import pandas as pd\n","tmp_df = pd.read_excel(\"/content/Load-data/Data_à¸ªà¸–à¸²à¸™à¸µà¸Šà¸²à¸£à¹Œà¸ˆ/à¸£à¸²à¸¢à¸‡à¸²à¸™à¸ªà¸£à¸¸à¸›-Demand-à¸£à¸²à¸¢à¸§à¸±à¸™-à¸ªà¸–à¸²à¸™à¸µà¸Šà¸²à¸£à¹Œà¸ˆ-01-2024.xlsx\")"],"metadata":{"id":"SGxtOT7ukUfi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from tqdm import tqdm\n","from multiprocessing import Pool, cpu_count\n","\n","# Define your cleaning function\n","def clean_header_and_drop_unuse_row(tmp_df):\n","    tmp_df.columns = tmp_df.iloc[0]\n","    tmp_df = tmp_df[1:]\n","    tmp_df = tmp_df.reset_index(drop=True)\n","    if 'Date' in tmp_df.columns:\n","        tmp_df = tmp_df[~pd.isna(tmp_df['Date'])]\n","    return tmp_df\n","\n","# Helper function to process a single file\n","def process_file(file_info):\n","    file_path, rel_path = file_info\n","\n","    try:\n","        tmp_df = pd.read_excel(file_path)\n","        cleaned_df = clean_header_and_drop_unuse_row(tmp_df)\n","\n","        # Construct new CSV path\n","        output_path = os.path.join(\"/content/cleaned_data\", rel_path).replace(\".xlsx\", \".csv\")\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","\n","        # Save to CSV\n","        cleaned_df.to_csv(output_path, index=False)\n","        return f\"âœ… Processed: {file_path}\"\n","    except Exception as e:\n","        return f\"âŒ Error with {file_path}: {str(e)}\"\n","\n","# Gather all .xlsx files with relative paths\n","xlsx_files = []\n","root_dir = \"/content/Load-data\"\n","\n","for subdir, _, files in os.walk(root_dir):\n","    for file in files:\n","        if file.endswith(\".xlsx\"):\n","            full_path = os.path.join(subdir, file)\n","            rel_path = os.path.relpath(full_path, root_dir)\n","            xlsx_files.append((full_path, rel_path))\n","\n","# Run in parallel using Pool\n","with Pool(cpu_count()) as pool:\n","    results = list(tqdm(pool.imap_unordered(process_file, xlsx_files), total=len(xlsx_files)))\n","\n","# Optional: Print summary\n","for res in results:\n","    print(res)"],"metadata":{"id":"mxs4Dbz8k0oq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocess"],"metadata":{"id":"VQxF7TKEssUr"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import re\n","from datetime import datetime\n","from tqdm import tqdm\n","from multiprocessing import Pool, cpu_count\n","\n","# Updated function\n","def preprocess_convert_datatype_with_date(tmp_df, filename):\n","    # Extract MM-YYYY from filename\n","    match = re.search(r\"(\\d{2})-(\\d{4})\", filename)\n","    if not match:\n","        raise ValueError(f\"Cannot extract date from filename: {filename}\")\n","\n","    start_month = int(match.group(1))\n","    start_year = int(match.group(2))\n","\n","    # Generate datetime range\n","    num_days = len(tmp_df)\n","    date_range = pd.date_range(start=datetime(start_year, start_month, 1), periods=num_days, freq='D')\n","    tmp_df['Date'] = date_range\n","\n","    # Convert all other columns to numeric\n","    time_cols = [col for col in tmp_df.columns if col != \"Date\"]\n","    tmp_df[time_cols] = tmp_df[time_cols].apply(pd.to_numeric, errors='coerce')\n","\n","    return tmp_df\n","\n","# Wrapper for parallel processing\n","def process_csv_file(file_info):\n","    file_path, rel_path = file_info\n","\n","    try:\n","        tmp_df = pd.read_csv(file_path)\n","        processed_df = preprocess_convert_datatype_with_date(tmp_df, os.path.basename(file_path))\n","\n","        # Extract station name from relative path\n","        station_name = os.path.normpath(rel_path).split(os.sep)[0]\n","        processed_df.insert(0, 'station_name', station_name)  # Add as first column\n","\n","        # Save to new folder\n","        output_path = os.path.join(\"/content/preprocessed_data\", rel_path)\n","        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","        processed_df.to_csv(output_path, index=False)\n","        return f\"âœ… Processed: {file_path}\"\n","    except Exception as e:\n","        return f\"âŒ Error with {file_path}: {str(e)}\"\n","\n","# Collect files\n","csv_files = []\n","root_dir = \"/content/cleaned_data\"\n","\n","for subdir, _, files in os.walk(root_dir):\n","    for file in files:\n","        if file.endswith(\".csv\"):\n","            full_path = os.path.join(subdir, file)\n","            rel_path = os.path.relpath(full_path, root_dir)\n","            csv_files.append((full_path, rel_path))\n","\n","# Run in parallel\n","with Pool(cpu_count()) as pool:\n","    results = list(tqdm(pool.imap_unordered(process_csv_file, csv_files), total=len(csv_files)))\n","\n","# Optional: Print summary\n","for res in results:\n","    print(res)"],"metadata":{"id":"5mdHlXk2lHDx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EDA"],"metadata":{"id":"U9TpfFVPxDa-"}},{"cell_type":"code","source":["!unzip \"/content/Roboto Prompt.zip\" -d \"font\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cxi-ld_ZxiAs","executionInfo":{"status":"ok","timestamp":1752145517520,"user_tz":-420,"elapsed":8851,"user":{"displayName":"FIFA AK","userId":"13407782711191647620"}},"outputId":"de94c35d-5fc3-4eea-ff7e-7de5ab3a4bd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/Roboto Prompt.zip\n","replace font/Roboto/OFL.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace font/Roboto/README.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"]}]},{"cell_type":"code","source":["# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# ğŸ“Œ Set Thai Font: Prompt-Regular.ttf\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","# Load custom font\n","font_path = \"/content/font/Prompt/Prompt-Regular.ttf\"\n","custom_font = fm.FontProperties(fname=font_path)\n","plt.rcParams['font.family'] = custom_font.get_name()\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# ğŸ“ EDA by Subfolder (station)\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","import pandas as pd\n","import os\n","from glob import glob\n","from tqdm import tqdm\n","\n","# Folder containing preprocessed data\n","preprocessed_root = \"/content/preprocessed_data\"\n","\n","# Step 1: Group CSVs by station folder\n","folder_groups = {}\n","for csv_file in glob(os.path.join(preprocessed_root, '**', '*.csv'), recursive=True):\n","    station_folder = os.path.normpath(csv_file).split(os.sep)[-2]  # Get station name from parent folder\n","    folder_groups.setdefault(station_folder, []).append(csv_file)\n","\n","# Step 2: EDA loop\n","for station, files in folder_groups.items():\n","    print(f\"\\nğŸ“Š EDA à¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¸–à¸²à¸™à¸µ: {station}\")\n","\n","    # Combine CSVs for this station\n","    dfs = [pd.read_csv(f) for f in files]\n","    station_df = pd.concat(dfs, ignore_index=True)\n","\n","    # Show info\n","    print(\"âœ… à¸ˆà¸³à¸™à¸§à¸™à¹à¸–à¸§à¹à¸¥à¸°à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ:\", station_df.shape)\n","    print(\"ğŸ“… à¸Šà¹ˆà¸§à¸‡à¸§à¸±à¸™à¸—à¸µà¹ˆ:\", station_df['Date'].min(), \"â†’\", station_df['Date'].max())\n","    print(\"ğŸ§¾ à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ:\", station_df.columns.tolist())\n","\n","    # Missing value summary\n","    print(\"â— à¸„à¹ˆà¸² Missing:\\n\", station_df.isna().sum())\n","\n","    # Total usage calculation\n","    time_cols = [col for col in station_df.columns if col not in ['Date', 'station_name']]\n","    station_df['TotalUsage'] = station_df[time_cols].sum(axis=1)\n","\n","    station_df['Date'] = pd.to_datetime(station_df['Date'])\n","    daily_usage = station_df.groupby('Date')['TotalUsage'].sum()\n","\n","    # Plot daily usage\n","    plt.figure(figsize=(10, 4))\n","    plt.plot(daily_usage.index, daily_usage.values)\n","    plt.title(f\"{station} - à¸à¸²à¸£à¹ƒà¸Šà¹‰à¹„à¸Ÿà¸Ÿà¹‰à¸²à¸£à¸§à¸¡à¸£à¸²à¸¢à¸§à¸±à¸™\", fontsize=14, fontproperties=custom_font)\n","    plt.xlabel(\"à¸§à¸±à¸™à¸—à¸µà¹ˆ\", fontproperties=custom_font)\n","    plt.ylabel(\"à¸à¸²à¸£à¹ƒà¸Šà¹‰à¹„à¸Ÿà¸Ÿà¹‰à¸² (kW)\", fontproperties=custom_font)\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"YEpRJ1DfxF-_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Decomposition"],"metadata":{"id":"pAzltC3c0E5w"}},{"cell_type":"code","source":["# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# ğŸ“Œ Suppress Warnings\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# ğŸ“Œ Set Thai Font: Prompt-Regular.ttf\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","import matplotlib.dates as mdates\n","\n","font_path = \"/content/font/Prompt/Prompt-Regular.ttf\"\n","custom_font = fm.FontProperties(fname=font_path)\n","plt.rcParams['font.family'] = custom_font.get_name()\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# ğŸ“¦ Imports\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","import pandas as pd\n","import os\n","from glob import glob\n","from tqdm import tqdm\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# ğŸ“ Seasonal Decomposition by Station\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","preprocessed_root = \"/content/preprocessed_data\"\n","\n","# Step 1: Group CSVs by station folder\n","folder_groups = {}\n","for csv_file in glob(os.path.join(preprocessed_root, '**', '*.csv'), recursive=True):\n","    station_folder = os.path.normpath(csv_file).split(os.sep)[-2]\n","    folder_groups.setdefault(station_folder, []).append(csv_file)\n","\n","# Step 2: EDA + Decomposition\n","for station, files in folder_groups.items():\n","    print(f\"\\nğŸ“Š EDA + Decomposition à¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¸–à¸²à¸™à¸µ: {station}\")\n","\n","    # Combine CSVs\n","    dfs = [pd.read_csv(f) for f in files]\n","    station_df = pd.concat(dfs, ignore_index=True)\n","\n","    # Parse dates\n","    station_df['Date'] = pd.to_datetime(station_df['Date'])\n","\n","    # Set 'Date' as index for easier time series operations\n","    station_df = station_df.set_index('Date').sort_index()\n","\n","    # Calculate TotalUsage\n","    time_cols = [col for col in station_df.columns if col not in ['station_name']] # 'Date' is now index\n","    station_df['TotalUsage'] = station_df[time_cols].sum(axis=1)\n","\n","    # Group daily usage and resample to daily frequency, summing usage for each day\n","    # This step will create NaNs for any missing days automatically\n","    daily_usage = station_df['TotalUsage'].resample('D').sum()\n","\n","    # Handle NaNs: For decomposition, it's critical to have no NaNs.\n","    # ffill is acceptable for short gaps, but be aware of its limitations.\n","    # If you have long periods of missing data, consider other imputation methods\n","    # or acknowledge that decomposition might be less accurate.\n","    daily_usage = daily_usage.fillna(method='ffill')\n","    # An alternative for decomposition if ffill is not desired is to interpolate:\n","    # daily_usage = daily_usage.interpolate(method='time') # This might be better for some cases\n","\n","    # Check for remaining NaNs after fillna (should be none)\n","    if daily_usage.isnull().any():\n","        print(f\"Warning: NaNs still present in {station} daily_usage after ffill. Decomposition might fail or be inaccurate.\")\n","        continue # Skip to the next station if decomposition won't work\n","\n","    # Seasonal decomposition (daily series, assumes 7-day weekly seasonality)\n","    # Ensure the series is long enough for the period. For period=7, you need at least 2*7 = 14 data points.\n","    if len(daily_usage) < 2 * 7: # Minimum 2 full cycles for reliable decomposition\n","        print(f\"Skipping decomposition for {station}: Not enough data points ({len(daily_usage)}) for period=7.\")\n","        continue\n","\n","\n","    result = seasonal_decompose(daily_usage, model='additive', period=7)\n","\n","    fig = result.plot()\n","    fig.set_size_inches(16, 10)  # Wider and taller\n","\n","    # Title in Thai with font\n","    fig.suptitle(f'{station} - à¸à¸²à¸£à¹à¸¢à¸à¸­à¸‡à¸„à¹Œà¸›à¸£à¸°à¸à¸­à¸šà¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸§à¸¥à¸²', fontsize=18, fontproperties=custom_font, y=1.02)\n","\n","    # Improve tick frequency and format\n","    for ax in fig.axes:\n","        ax.set_ylabel(\"\", fontproperties=custom_font)\n","        ax.tick_params(axis='x', labelrotation=45)\n","\n","        # Set major ticks to weekly intervals for better detail\n","        ax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=1))  # show every week\n","        ax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%b'))     # format: 01-Jul\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"zIitcbx0zxQ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocess"],"metadata":{"id":"RpWEs5Q0xExb"}},{"cell_type":"code","source":["# After processing is done, concatenate all processed files\n","all_data = []\n","\n","output_root = \"/content/preprocessed_data\"\n","\n","for subdir, _, files in os.walk(output_root):\n","    for file in files:\n","        if file.endswith(\".csv\"):\n","            file_path = os.path.join(subdir, file)\n","            try:\n","                df = pd.read_csv(file_path)\n","                all_data.append(df)\n","            except Exception as e:\n","                print(f\"âŒ Failed to read {file_path}: {e}\")\n","\n","# Concatenate all data\n","if all_data:\n","    all_data_df = pd.concat(all_data, ignore_index=True)\n","    all_data_df.to_csv(\"/content/all_data_df.csv\", index=False)\n","    print(\"âœ… All data concatenated and saved to /content/all_data_df.csv\")\n","else:\n","    print(\"âš ï¸ No data was loaded for concatenation.\")\n","import re\n","\n","# Identify time columns (HH:MM format)\n","time_columns = [col for col in all_data_df.columns if re.match(r\"^\\d{1,2}:\\d{2}$\", str(col))]\n","\n","# Melt the DataFrame to long format\n","long_df = all_data_df.melt(\n","    id_vars=['station_name', 'Date'],\n","    value_vars=time_columns,\n","    var_name='Time',\n","    value_name='Electricity(kW)'\n",")\n","\n","# Combine 'Date' and 'Time' into full datetime\n","long_df['Date'] = pd.to_datetime(long_df['Date'].astype(str) + ' ' + long_df['Time'])\n","\n","# Drop 'Time' column\n","long_df.drop(columns=['Time'], inplace=True)\n","\n","# Sort by station_name first, then by Date\n","long_df.sort_values(by=['station_name', 'Date'], inplace=True)\n","\n","# Save to CSV\n","long_df.to_csv('/content/all_data_timeseries.csv', index=False)\n","print(\"âœ… Time series data saved and sorted by station_name > Date to /content/all_data_timeseries.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uRXnGzljv1LY","executionInfo":{"status":"ok","timestamp":1755678386472,"user_tz":-420,"elapsed":1231,"user":{"displayName":"FIFA AK","userId":"13407782711191647620"}},"outputId":"cbcfa207-863c-4d1c-e20b-faef79f80748"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… All data concatenated and saved to /content/all_data_df.csv\n","âœ… Time series data saved and sorted by station_name > Date to /content/all_data_timeseries.csv\n"]}]},{"cell_type":"markdown","source":["## Define Weight"],"metadata":{"id":"xij_U2YS-Bwu"}},{"cell_type":"code","source":["import pandas as pd\n","\n","def build_station_weights(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Compute normalized reverse weights per station using record counts:\n","      - weight = max_count / count\n","      - so the station with the most records has weight == 1\n","      - stations with fewer records get weights > 1\n","\n","    Returns a DataFrame with columns:\n","      station_name, normalized_reverse_weight\n","    \"\"\"\n","    station_counts = df['station_name'].value_counts()\n","\n","    # Normalize so max count has weight = 1\n","    max_count = station_counts.max()\n","    normalized_reverse_weights = max_count / station_counts\n","\n","    # Convert to DataFrame for easier viewing\n","    station_weights_df = normalized_reverse_weights.reset_index()\n","    station_weights_df.columns = ['station_name', 'normalized_reverse_weight']\n","    return station_weights_df\n","long_df = long_df[long_df['station_name']!= 'Data_à¸­à¸²à¸„à¸²à¸£à¸§à¸´à¸—à¸¢à¸™à¸´à¹€à¸§à¸¨à¸™à¹Œ']\n","station_weights_df =build_station_weights(long_df)"],"metadata":{"id":"E4z8xMs56jRq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["station_weights_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"OLZQdE9T7Far","executionInfo":{"status":"ok","timestamp":1755667418158,"user_tz":-420,"elapsed":44,"user":{"displayName":"FIFA AK","userId":"13407782711191647620"}},"outputId":"11070726-3e75-4d5e-e898-8469be36414b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             station_name  normalized_reverse_weight\n","0         Data_à¸ªà¸–à¸²à¸™à¸µà¸Šà¸²à¸£à¹Œà¸ˆ                   1.000000\n","1     Data_à¸­à¸²à¸„à¸²à¸£à¸ˆà¸²à¸¡à¸ˆà¸¸à¸£à¸µ 9                   1.000000\n","2   Data_à¸­à¸²à¸„à¸²à¸£à¸ˆà¸¸à¸¥à¸ˆà¸±à¸à¸£à¸à¸‡à¸©à¹Œ                   1.002786\n","3  Data_à¸­à¸²à¸„à¸²à¸£à¸šà¸£à¸¡à¸£à¸²à¸Šà¸à¸¸à¸¡à¸²à¸£à¸µ                   1.002786\n","4      Data_à¸­à¸²à¸„à¸²à¸£à¸ˆà¸²à¸¡à¸ˆà¸¸à¸£à¸µ4                   1.094225"],"text/html":["\n","  <div id=\"df-c01054fb-5301-4099-b0a3-f3093da8d043\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>station_name</th>\n","      <th>normalized_reverse_weight</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Data_à¸ªà¸–à¸²à¸™à¸µà¸Šà¸²à¸£à¹Œà¸ˆ</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Data_à¸­à¸²à¸„à¸²à¸£à¸ˆà¸²à¸¡à¸ˆà¸¸à¸£à¸µ 9</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Data_à¸­à¸²à¸„à¸²à¸£à¸ˆà¸¸à¸¥à¸ˆà¸±à¸à¸£à¸à¸‡à¸©à¹Œ</td>\n","      <td>1.002786</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Data_à¸­à¸²à¸„à¸²à¸£à¸šà¸£à¸¡à¸£à¸²à¸Šà¸à¸¸à¸¡à¸²à¸£à¸µ</td>\n","      <td>1.002786</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Data_à¸­à¸²à¸„à¸²à¸£à¸ˆà¸²à¸¡à¸ˆà¸¸à¸£à¸µ4</td>\n","      <td>1.094225</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c01054fb-5301-4099-b0a3-f3093da8d043')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c01054fb-5301-4099-b0a3-f3093da8d043 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c01054fb-5301-4099-b0a3-f3093da8d043');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-876a2485-bea5-4d13-a115-e2215b141b3b\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-876a2485-bea5-4d13-a115-e2215b141b3b')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-876a2485-bea5-4d13-a115-e2215b141b3b button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_ae8e463d-f946-4dbd-bb42-6d0d6e334d84\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('station_weights_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_ae8e463d-f946-4dbd-bb42-6d0d6e334d84 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('station_weights_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"station_weights_df","summary":"{\n  \"name\": \"station_weights_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"station_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Data_\\u0e2d\\u0e32\\u0e04\\u0e32\\u0e23\\u0e08\\u0e32\\u0e21\\u0e08\\u0e38\\u0e23\\u0e35 9\",\n          \"Data_\\u0e2d\\u0e32\\u0e04\\u0e32\\u0e23\\u0e08\\u0e32\\u0e21\\u0e08\\u0e38\\u0e23\\u0e354\",\n          \"Data_\\u0e2d\\u0e32\\u0e04\\u0e32\\u0e23\\u0e08\\u0e38\\u0e25\\u0e08\\u0e31\\u0e01\\u0e23\\u0e1e\\u0e07\\u0e29\\u0e4c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"normalized_reverse_weight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0415391621944966,\n        \"min\": 1.0,\n        \"max\": 1.094224924012158,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          1.0027855153203342,\n          1.094224924012158\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["## Experiment [Fill mising value]"],"metadata":{"id":"cbmXMa55si81"}},{"cell_type":"code","source":["long_df.loc[long_df['Electricity(kW)'] < 0, 'Electricity(kW)'] = 0"],"metadata":{"id":"5BDUFvTjzGI7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Split train,valid and test"],"metadata":{"id":"rg9u9_6Xt8bK"}},{"cell_type":"code","source":["# Define ratios\n","train_ratio = 0.8\n","test_ratio = 0.2  # Optional, just for clarity (1 - train_ratio)\n","\n","# Create empty lists to collect per-station splits\n","train_list = []\n","test_list = []\n","\n","# Split per station\n","for station, station_df in long_df.groupby('station_name'):\n","    station_df = station_df.sort_values('Date')\n","    n = len(station_df)\n","\n","    train_end = int(n * train_ratio)\n","\n","    train_list.append(station_df.iloc[:train_end])\n","    test_list.append(station_df.iloc[train_end:])\n","\n","# Combine all stations back into global sets\n","train_df = pd.concat(train_list).reset_index(drop=True)\n","test_df = pd.concat(test_list).reset_index(drop=True)\n","\n","# Save to CSV\n","train_df.to_csv('/content/train_timeseries.csv', index=False)\n","test_df.to_csv('/content/test_timeseries.csv', index=False)\n","\n","print(\"âœ… Split completed:\")\n","print(f\"Train set size: {len(train_df)} rows\")\n","print(f\"Test set size: {len(test_df)} rows\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AtDIorByqvOn","executionInfo":{"status":"ok","timestamp":1755678387123,"user_tz":-420,"elapsed":522,"user":{"displayName":"FIFA AK","userId":"13407782711191647620"}},"outputId":"ddb352df-86dd-449b-be01-8ed41954c304"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Split completed:\n","Train set size: 135705 rows\n","Test set size: 33927 rows\n"]}]},{"cell_type":"markdown","source":["## Modeling"],"metadata":{"id":"iONC3kQqzH9V"}},{"cell_type":"code","source":["!pip install -U autogluon"],"metadata":{"id":"gyWkWyHszI8g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Experiment"],"metadata":{"id":"7QDA1vFTIePV"}},{"cell_type":"code","source":["# === Random Forest Tabular + Weighted WAPE (AutoGluon) ===\n","import pandas as pd\n","import numpy as np\n","from autogluon.tabular import TabularPredictor\n","\n","RANDOM_SEED = 42\n","LABEL = \"Electricity(kW)\"\n","DATA_TRAIN = \"/content/train_timeseries.csv\"\n","DATA_TEST  = \"/content/test_timeseries.csv\"\n","\n","# 1) Load\n","train_df = pd.read_csv(DATA_TRAIN)\n","test_df  = pd.read_csv(DATA_TEST)\n","\n","# 1.1) Clean non-finite label values (if label exists in df)\n","def clean_non_finite(df: pd.DataFrame, label_col: str) -> pd.DataFrame:\n","    if label_col in df.columns:\n","        return df[np.isfinite(df[label_col])].copy()\n","    return df.copy()\n","\n","train_df = clean_non_finite(train_df, LABEL)\n","test_df  = clean_non_finite(test_df, LABEL)\n","\n","# 1.2) Parse datetime\n","train_df[\"Date\"] = pd.to_datetime(train_df[\"Date\"])\n","test_df[\"Date\"]  = pd.to_datetime(test_df[\"Date\"])\n","\n","# 2) Feature engineering (basic time features; keep it tabular)\n","def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n","    df = df.sort_values([\"station_name\", \"Date\"]).copy()\n","    df[\"hour\"] = df[\"Date\"].dt.hour\n","    df[\"dayofweek\"] = df[\"Date\"].dt.dayofweek\n","    df[\"month\"] = df[\"Date\"].dt.month\n","    return df\n","\n","train_tab = add_time_features(train_df)\n","test_tab  = add_time_features(test_df)\n","\n","# (No lags/rollings here, so dropna is optional; keep for safety)\n","train_tab = train_tab.dropna().reset_index(drop=True)\n","test_tab  = test_tab.dropna().reset_index(drop=True)\n","\n","# 3) Station weights (normalized reverse weight: max_count / count)\n","def build_station_weights(df: pd.DataFrame) -> pd.Series:\n","    \"\"\"\n","    Returns a Series: index=station_name, value=normalized_reverse_weight\n","      - weight = max_count / count  (most-sampled station â†’ 1.0, fewer samples â†’ >1.0)\n","    \"\"\"\n","    counts = df[\"station_name\"].value_counts()\n","    max_count = counts.max()\n","    weights = (max_count / counts).astype(float)\n","    weights.name = \"normalized_reverse_weight\"\n","    return weights\n","\n","# Use TRAIN distribution to define weights\n","station_weights = build_station_weights(train_tab)  # pd.Series\n","\n","# 4) Fit AutoGluon Tabular with ONLY Random Forest\n","features = [c for c in train_tab.columns if c not in [\"Date\", LABEL]]\n","predictor = TabularPredictor(\n","    label=LABEL,\n","    path=\"AutogluonTabularModels_RF_only\",\n","    eval_metric=\"mean_squared_error\",\n","    problem_type=\"regression\"\n",").fit(\n","    train_data=train_tab[features + [LABEL]],\n","    presets=None,\n","    time_limit=3600,\n","    hyperparameters={\n","        # 'RF' = RandomForest in AutoGluon Tabular\n","        \"RF\": {\n","            \"random_state\": RANDOM_SEED\n","        }\n","    }\n",")\n","\n","# 5) Predict on test\n","y_pred = predictor.predict(test_tab[features])\n","\n","# 6) Evaluate Weighted WAPE\n","#    WAPE = sum_i w_i * sum_t |y - yhat|  /  sum_i w_i * sum_t |y|\n","# NOTE: assumes test set HAS the label column\n","if LABEL not in test_tab.columns:\n","    raise ValueError(\"Test set has no label column; cannot compute WAPE.\")\n","\n","def weighted_wape(y_true: pd.Series, y_pred: pd.Series, stations: pd.Series, station_w: pd.Series) -> float:\n","    # map station -> weight; missing stations default to 1.0\n","    w = stations.map(station_w).fillna(1.0).to_numpy(dtype=float)\n","    ae = (y_true - y_pred).abs().to_numpy(dtype=float)\n","    abs_y = y_true.abs().to_numpy(dtype=float)\n","    num = (w * ae).sum()\n","    den = (w * abs_y).sum()\n","    # If all actuals are zero, fall back to unweighted MAE to avoid div/0\n","    return num / den if den != 0 else ae.mean()\n","\n","# Prepare aligned Series\n","y_true_ser = test_tab[LABEL].reset_index(drop=True)\n","y_pred_ser = y_pred.reset_index(drop=True)\n","station_ser = test_tab[\"station_name\"].reset_index(drop=True)\n","\n","wwape = weighted_wape(y_true_ser, y_pred_ser, station_ser, station_weights)\n","\n","# Also print plain WAPE for reference (no weights)\n","def plain_wape(y_true: pd.Series, y_pred: pd.Series) -> float:\n","    num = (y_true - y_pred).abs().sum()\n","    den = y_true.abs().sum()\n","    return float(num / den) if den != 0 else float((y_true - y_pred).abs().mean())\n","\n","wape_plain = plain_wape(y_true_ser, y_pred_ser)\n","\n","print(\"\\n=== EVAL (RandomForest only) ===\")\n","print(f\"Weighted WAPE: {wwape:.6f}\")\n","print(f\"Plain WAPE   : {wape_plain:.6f}\")\n","\n","# Optionally: also MSE/MAE quick check\n","mse = np.mean((y_true_ser - y_pred_ser) ** 2)\n","mae = np.mean((y_true_ser - y_pred_ser).abs())\n","print(f\"MAE: {mae:.6f}\")\n","print(f\"MSE: {mse:.6f}\")\n","\n","# If you still want a leaderboard (single model), you can pass the test set\n","# but AutoGluon's leaderboard won't know about the custom weighted metric:\n","# lb = predictor.leaderboard(test_tab[features + [LABEL]], silent=True)\n","# print(lb)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uRKjK-WVIfeD","executionInfo":{"status":"ok","timestamp":1755684832179,"user_tz":-420,"elapsed":39980,"user":{"displayName":"FIFA AK","userId":"13407782711191647620"}},"outputId":"a8fdb7b3-3249-462f-fd08-0c627f00a4f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Verbosity: 2 (Standard Logging)\n","=================== System Info ===================\n","AutoGluon Version:  1.4.0\n","Python Version:     3.12.11\n","Operating System:   Linux\n","Platform Machine:   x86_64\n","Platform Version:   #1 SMP PREEMPT_DYNAMIC Sun Mar 30 16:01:29 UTC 2025\n","CPU Count:          2\n","Memory Avail:       10.04 GB / 12.67 GB (79.2%)\n","Disk Space Avail:   66.46 GB / 112.64 GB (59.0%)\n","===================================================\n","No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n","\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n","\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n","\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n","\tpresets='high'    : Strong accuracy with fast inference speed.\n","\tpresets='good'    : Good accuracy with very fast inference speed.\n","\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n","Beginning AutoGluon training ... Time limit = 3600s\n","AutoGluon will save models to \"/content/AutogluonTabularModels_RF_only\"\n","Train Data Rows:    135663\n","Train Data Columns: 4\n","Label Column:       Electricity(kW)\n","Problem Type:       regression\n","Preprocessing data ...\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    10295.24 MB\n","\tTrain Data (Original)  Memory Usage: 18.63 MB (0.2% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\t\tFitting CategoryFeatureGenerator...\n","\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('int', [])    : 3 | ['hour', 'dayofweek', 'month']\n","\t\t('object', []) : 1 | ['station_name']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('category', []) : 1 | ['station_name']\n","\t\t('int', [])      : 3 | ['hour', 'dayofweek', 'month']\n","\t0.3s = Fit runtime\n","\t4 features in original data used to generate 4 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 1.68 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.3s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","Automatically generating train/validation split with holdout_frac=0.01842801648201794, Train Rows: 133163, Val Rows: 2500\n","User-specified model hyperparameters to be fit:\n","{\n","\t'RF': [{'random_state': 42}],\n","}\n","Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n","Fitting model: RandomForest ... Training model for up to 3599.70s of the 3599.70s of remaining time.\n","\tFitting with cpus=2, gpus=0, mem=0.1/10.0 GB\n","\t-3095.1754\t = Validation score   (-mean_squared_error)\n","\t35.08s\t = Training   runtime\n","\t0.25s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 3563.50s of remaining time.\n","\tEnsemble Weights: {'RandomForest': 1.0}\n","\t-3095.1754\t = Validation score   (-mean_squared_error)\n","\t0.0s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 36.54s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 9973.3 rows/s (2500 batch size)\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/AutogluonTabularModels_RF_only\")\n"]},{"output_type":"stream","name":"stdout","text":["\n","=== EVAL (RandomForest only) ===\n","Weighted WAPE: 0.367256\n","Plain WAPE   : 0.367232\n","MAE: 38.165514\n","MSE: 5927.953031\n"]}]}]}