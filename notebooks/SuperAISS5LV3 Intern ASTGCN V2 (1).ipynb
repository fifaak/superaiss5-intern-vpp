{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn pandas numpy torchview visualtorch gdown torch-geometric-temporal torch-cluster # then restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9ekDThljjj0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!gdown --folder https://drive.google.com/drive/folders/1dydbU9HlSIgGQBzYMLogDNI27uO6wga7?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABStU3J8kQ9D"
   },
   "source": [
    "## Load & Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-15T16:36:45.683516Z",
     "iopub.status.busy": "2025-07-15T16:36:45.683237Z",
     "iopub.status.idle": "2025-07-15T16:36:50.783028Z",
     "shell.execute_reply": "2025-07-15T16:36:50.781903Z",
     "shell.execute_reply.started": "2025-07-15T16:36:45.683494Z"
    },
    "id": "qldgzd415jX5",
    "outputId": "59fca88e-18c5-463f-8a8c-da75de4415fe",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:02<00:00, 28.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-09-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-01-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-12-2023.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-08-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-07-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-04-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-10-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-02-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-03-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-05-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-06-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-11-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-11-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-01-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-07-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-08-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-12-2023.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-06-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-02-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-03-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-05-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-04-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-10-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-02-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-09-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-03-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-11-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-12-2023.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-10-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-07-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-05-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-09-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-06-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-01-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-04-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-11-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-08-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-06-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-08-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-12-2023.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-07-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-04-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-05-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-03-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-09-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-02-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-01-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-10-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-09-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-04-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-07-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-06-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-03-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-11-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-08-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-10-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-01-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-05-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-04-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-02-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-05-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-06-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-07-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-03-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-11-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-08-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-01-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-12-2023.xlsx\n",
      "✅ Excel Processed: Load-data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-09-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-02-2024.xlsx\n",
      "✅ Excel Processed: Load-data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-10-2024.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 71/71 [00:00<00:00, 86.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-12-2023.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-11-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-01-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-03-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-04-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-10-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-06-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-05-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-08-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-07-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-09-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี 9/รายงานสรุป-Demand-รายวัน-อาคารจามจุรี9-02-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-02-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-11-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-01-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-12-2023.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-09-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-08-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-10-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-05-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-07-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-06-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-04-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจุลจักรพงษ์/รายงานสรุป-Demand-รายวัน-อาคารจุลจักรพงษ์-03-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-10-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-04-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-11-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-12-2023.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-07-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-02-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-09-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-06-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-05-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-03-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-08-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารวิทยนิเวศน์/รายงานสรุป-Demand-รายวัน-อาคารวิทยนิเวศน์-01-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-05-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-08-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-03-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-12-2023.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-09-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-04-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-10-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-06-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-11-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-01-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-07-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารบรมราชกุมารี/รายงานสรุป-Demand-รายวัน-อาคารบรมราชกุมารี-02-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-08-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-05-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-01-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-09-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-04-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-02-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-10-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-03-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-11-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-06-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_อาคารจามจุรี4/รายงานสรุป-Demand-รายวัน-จามจุรี-4-07-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-03-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-07-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-06-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-10-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-02-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-05-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-09-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-11-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-08-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-12-2023.csv\n",
      "✅ CSV Processed: cleaned_data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-01-2024.csv\n",
      "✅ CSV Processed: cleaned_data/Data_สถานีชาร์จ/รายงานสรุป-Demand-รายวัน-สถานีชาร์จ-04-2024.csv\n",
      "✅ Wide-format saved to all_data_df.csv\n",
      "✅ Long-format saved to all_data_timeseries.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# --------- CONFIGURATION ---------\n",
    "ROOT_XLSX_DIR = \"Load-data\"\n",
    "CLEANED_CSV_DIR = \"cleaned_data\"\n",
    "PREPROCESSED_CSV_DIR = \"preprocessed_data\"\n",
    "FINAL_WIDE_CSV = \"all_data_df.csv\"\n",
    "FINAL_LONG_CSV = \"all_data_timeseries.csv\"\n",
    "# ---------------------------------\n",
    "\n",
    "def clean_header_and_drop_unused_rows(tmp_df):\n",
    "    tmp_df.columns = tmp_df.iloc[0]\n",
    "    tmp_df = tmp_df[1:].reset_index(drop=True)\n",
    "    if 'Date' in tmp_df.columns:\n",
    "        tmp_df = tmp_df[~pd.isna(tmp_df['Date'])]\n",
    "    return tmp_df\n",
    "\n",
    "def process_excel_file(file_info):\n",
    "    file_path, rel_path = file_info\n",
    "    try:\n",
    "        tmp_df = pd.read_excel(file_path)\n",
    "        cleaned_df = clean_header_and_drop_unused_rows(tmp_df)\n",
    "        output_path = os.path.join(CLEANED_CSV_DIR, rel_path).replace(\".xlsx\", \".csv\")\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        cleaned_df.to_csv(output_path, index=False)\n",
    "        return f\"✅ Excel Processed: {file_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Excel Error in {file_path}: {str(e)}\"\n",
    "\n",
    "def preprocess_and_add_datetime(tmp_df, filename):\n",
    "    match = re.search(r\"(\\d{2})-(\\d{4})\", filename)\n",
    "    if not match:\n",
    "        raise ValueError(f\"❌ Cannot extract date from filename: {filename}\")\n",
    "\n",
    "    start_month = int(match.group(1))\n",
    "    start_year = int(match.group(2))\n",
    "    tmp_df = tmp_df.reset_index(drop=True)\n",
    "\n",
    "    date_range = pd.date_range(start=datetime(start_year, start_month, 1), periods=len(tmp_df), freq='D')\n",
    "    tmp_df['Date'] = date_range\n",
    "\n",
    "    time_cols = [col for col in tmp_df.columns if col != 'Date']\n",
    "    tmp_df[time_cols] = tmp_df[time_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    return tmp_df\n",
    "\n",
    "def process_csv_file(file_info):\n",
    "    file_path, rel_path = file_info\n",
    "    try:\n",
    "        tmp_df = pd.read_csv(file_path)\n",
    "        processed_df = preprocess_and_add_datetime(tmp_df, os.path.basename(file_path))\n",
    "\n",
    "        station_name = os.path.normpath(rel_path).split(os.sep)[0]\n",
    "        processed_df.insert(0, 'station_name', station_name)\n",
    "\n",
    "        output_path = os.path.join(PREPROCESSED_CSV_DIR, rel_path)\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        processed_df.to_csv(output_path, index=False)\n",
    "        return f\"✅ CSV Processed: {file_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ CSV Error in {file_path}: {str(e)}\"\n",
    "\n",
    "def gather_files(root_dir, extension):\n",
    "    files = []\n",
    "    for subdir, _, filenames in os.walk(root_dir):\n",
    "        for f in filenames:\n",
    "            if f.endswith(extension):\n",
    "                full = os.path.join(subdir, f)\n",
    "                rel = os.path.relpath(full, root_dir)\n",
    "                files.append((full, rel))\n",
    "    return files\n",
    "\n",
    "def concatenate_preprocessed_data(output_dir):\n",
    "    all_data = []\n",
    "    for subdir, _, files in os.walk(output_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                try:\n",
    "                    df = pd.read_csv(os.path.join(subdir, file))\n",
    "                    all_data.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Failed to read {file}: {e}\")\n",
    "    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "\n",
    "def convert_to_timeseries_long_format(df):\n",
    "    time_columns = [col for col in df.columns if re.match(r\"^\\d{1,2}:\\d{2}$\", str(col))]\n",
    "    long_df = df.melt(id_vars=['station_name', 'Date'], value_vars=time_columns,\n",
    "                      var_name='Time', value_name='Electricity(kW)')\n",
    "    long_df['Date'] = pd.to_datetime(long_df['Date'].astype(str) + ' ' + long_df['Time'])\n",
    "    long_df.drop(columns=['Time'], inplace=True)\n",
    "    long_df.sort_values(by=['station_name', 'Date'], inplace=True)\n",
    "    return long_df\n",
    "\n",
    "# ----------- MAIN EXECUTION FLOW -----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Clean Excel files to CSV\n",
    "    xlsx_files = gather_files(ROOT_XLSX_DIR, \".xlsx\")\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = list(tqdm(pool.imap_unordered(process_excel_file, xlsx_files), total=len(xlsx_files)))\n",
    "    for res in results:\n",
    "        print(res)\n",
    "\n",
    "    # Step 2: Preprocess cleaned CSVs\n",
    "    csv_files = gather_files(CLEANED_CSV_DIR, \".csv\")\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = list(tqdm(pool.imap_unordered(process_csv_file, csv_files), total=len(csv_files)))\n",
    "    for res in results:\n",
    "        print(res)\n",
    "\n",
    "    # Step 3: Concatenate all preprocessed CSVs\n",
    "    all_data_df = concatenate_preprocessed_data(PREPROCESSED_CSV_DIR)\n",
    "    if not all_data_df.empty:\n",
    "        all_data_df.to_csv(FINAL_WIDE_CSV, index=False)\n",
    "        print(f\"✅ Wide-format saved to {FINAL_WIDE_CSV}\")\n",
    "\n",
    "        # Step 4: Convert to long time series format\n",
    "        long_df = convert_to_timeseries_long_format(all_data_df)\n",
    "        long_df.to_csv(FINAL_LONG_CSV, index=False)\n",
    "        print(f\"✅ Long-format saved to {FINAL_LONG_CSV}\")\n",
    "    else:\n",
    "        print(\"⚠️ No data found for concatenation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xij_U2YS-Bwu"
   },
   "source": [
    "## Define Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T16:36:50.784994Z",
     "iopub.status.busy": "2025-07-15T16:36:50.784698Z",
     "iopub.status.idle": "2025-07-15T16:36:50.790460Z",
     "shell.execute_reply": "2025-07-15T16:36:50.789644Z",
     "shell.execute_reply.started": "2025-07-15T16:36:50.784965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the DataFrame\n",
    "station_weights_df = pd.DataFrame({\n",
    "    \"station_name\": [\n",
    "        \"Data_สถานีชาร์จ\",\n",
    "        \"Data_อาคารจามจุรี 9\",\n",
    "        \"Data_อาคารวิทยนิเวศน์\",\n",
    "        \"Data_อาคารจุลจักรพงษ์\",\n",
    "        \"Data_อาคารบรมราชกุมารี\",\n",
    "        \"Data_อาคารจามจุรี4\",\n",
    "    ],\n",
    "    \"normalized_reverse_weight\": [\n",
    "        1.000000,\n",
    "        1.000000,\n",
    "        1.000000,        1.002786,\n",
    "        1.002786,\n",
    "        1.094225,\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbmXMa55si81"
   },
   "source": [
    "## Experiment [Clean Data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T16:36:51.800916Z",
     "iopub.status.busy": "2025-07-15T16:36:51.800620Z",
     "iopub.status.idle": "2025-07-15T16:36:51.809050Z",
     "shell.execute_reply": "2025-07-15T16:36:51.808096Z",
     "shell.execute_reply.started": "2025-07-15T16:36:51.800894Z"
    },
    "id": "5BDUFvTjzGI7",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n",
      "  return op(a, b)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(long_df):\n",
    "    long_df.loc[long_df['Electricity(kW)'] < 0, 'Electricity(kW)'] = 0\n",
    "    return long_df\n",
    "# long_df_tmp = preprocess(long_df_new)\n",
    "long_df = preprocess(long_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rg9u9_6Xt8bK"
   },
   "source": [
    "## Split train,valid and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-15T16:36:54.166943Z",
     "iopub.status.busy": "2025-07-15T16:36:54.166648Z",
     "iopub.status.idle": "2025-07-15T16:36:54.253685Z",
     "shell.execute_reply": "2025-07-15T16:36:54.252906Z",
     "shell.execute_reply.started": "2025-07-15T16:36:54.166921Z"
    },
    "id": "AtDIorByqvOn",
    "outputId": "4dc58f30-8668-4c99-d584-e7c6be639763",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_train_test_data(long_df,long_df_new):\n",
    "    # Define ratios\n",
    "    train_ratio = 0.8\n",
    "    test_ratio = 0.2  # Optional, just for clarity (1 - train_ratio)\n",
    "    \n",
    "    # Create empty lists to collect per-station splits\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    # Split per station\n",
    "    for station, station_df in long_df_new.groupby('station_name'):\n",
    "        station_df = station_df.sort_values('Date')\n",
    "        n = len(station_df)\n",
    "    \n",
    "        train_end = int(n * train_ratio)\n",
    "    \n",
    "        train_list.append(station_df.iloc[:train_end])\n",
    "        test_list.append(station_df.iloc[train_end:])\n",
    "    \n",
    "    # Combine all stations back into global sets\n",
    "    train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "    # Create empty lists to collect per-station splits\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for station, station_df in long_df.groupby('station_name'):\n",
    "        station_df = station_df.sort_values('Date')\n",
    "        n = len(station_df)\n",
    "    \n",
    "        train_end = int(n * train_ratio)\n",
    "    \n",
    "        train_list.append(station_df.iloc[:train_end])\n",
    "        test_list.append(station_df.iloc[train_end:])\n",
    "    \n",
    "    test_df_new = pd.concat(test_list).reset_index(drop=True)\n",
    "    \n",
    "    return train_df,test_df_new\n",
    "train_df,test_df = split_train_test_data(long_df,long_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-15T16:36:56.487590Z",
     "iopub.status.busy": "2025-07-15T16:36:56.486893Z",
     "iopub.status.idle": "2025-07-15T16:36:56.519745Z",
     "shell.execute_reply": "2025-07-15T16:36:56.519097Z",
     "shell.execute_reply.started": "2025-07-15T16:36:56.487549Z"
    },
    "id": "O-ztRs2H6twN",
    "outputId": "4abec5f5-935e-454d-8ac9-db9bc07f6576",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 163353 entries, 0 to 163352\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   station_name     163353 non-null  object        \n",
      " 1   Date             163353 non-null  datetime64[ns]\n",
      " 2   Electricity(kW)  163231 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(1), object(1)\n",
      "memory usage: 3.7+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40839 entries, 0 to 40838\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   station_name     40839 non-null  object        \n",
      " 1   Date             40839 non-null  datetime64[ns]\n",
      " 2   Electricity(kW)  40839 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(1), object(1)\n",
      "memory usage: 957.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                 station_name                Date  Electricity(kW)\n",
       " 163348  Data_อาคารวิทยนิเวศน์ 2024-09-15 22:45:00              0.0\n",
       " 163349  Data_อาคารวิทยนิเวศน์ 2024-09-15 23:00:00              0.0\n",
       " 163350  Data_อาคารวิทยนิเวศน์ 2024-09-15 23:15:00              0.0\n",
       " 163351  Data_อาคารวิทยนิเวศน์ 2024-09-15 23:30:00              0.0\n",
       " 163352  Data_อาคารวิทยนิเวศน์ 2024-09-15 23:45:00              0.0,\n",
       "                 station_name                Date  Electricity(kW)\n",
       " 40834  Data_อาคารวิทยนิเวศน์ 2024-11-27 22:45:00             0.00\n",
       " 40835  Data_อาคารวิทยนิเวศน์ 2024-11-27 23:00:00             0.00\n",
       " 40836  Data_อาคารวิทยนิเวศน์ 2024-11-27 23:15:00             0.00\n",
       " 40837  Data_อาคารวิทยนิเวศน์ 2024-11-27 23:30:00             0.00\n",
       " 40838  Data_อาคารวิทยนิเวศน์ 2024-11-27 23:45:00             0.03,\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = {\n",
    "    \"Data_สถานีชาร์จ\": (13.73624, 100.52995), #Station_name, latitude,longitude\n",
    "    \"Data_อาคารจามจุรี4\": (13.73260, 100.53177),\n",
    "    \"Data_อาคารจามจุรี 9\": (13.73380, 100.53045),\n",
    "    \"Data_อาคารจุลจักรพงษ์\": (13.73684, 100.52852),\n",
    "    \"Data_อาคารบรมราชกุมารี\": (13.73800, 100.52905),\n",
    "    \"Data_อาคารวิทยนิเวศน์\": (13.73723, 100.53015),\n",
    "}\n",
    "train_df.tail(),test_df.tail(),train_df.info(),test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T17:31:16.284242Z",
     "iopub.status.busy": "2025-07-15T17:31:16.283902Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2582/925447085.py:121: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d752dbcb32429a819dae5745862c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 01:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2582/925447085.py:133: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 — Avg Loss: 15610.5403\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef6bbd35c6449968853d08be40fe8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 02:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 — Avg Loss: 8847.7999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e85fa8001244747978b22729681bbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 03:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 — Avg Loss: 6087.0939\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7ccf78d280455c85876c900d0dd6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 04:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 — Avg Loss: 5691.5266\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e7c2258c694034b0ad4e12fb701cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 05:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 — Avg Loss: 5238.5119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5cb1591c5e4bff860c82de2265f9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 06:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 — Avg Loss: 4940.6096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ff87d3c6b94740ae219b4cf0352223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 07:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 — Avg Loss: 5571.9517\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775d503f12bc4a80bd45681f634521b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 08:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 — Avg Loss: 5914.4908\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416d3e8d5729496aacb3c1583aa1f813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 09:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 — Avg Loss: 5287.7232\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4a3de108a649bf8d9cdcd54fe7727f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 — Avg Loss: 4648.4655\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b227672c33de43ac957a2141f816c09d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 — Avg Loss: 4457.2845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f931a69e6284a83b5cfb7137c5ac754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 — Avg Loss: 4339.9532\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b1dc8b5daf4b86a90fcd20a0fd5837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 — Avg Loss: 4337.7640\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6daa0c24a4b248b0abdaf96fb4070a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 — Avg Loss: 4507.5241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f67ffeadb874f429e0770f52d3dae9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 — Avg Loss: 4304.1267\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f22f9a646394a1cac457bf74fa5d106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 — Avg Loss: 4124.8091\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab50da987f18477f9cfc1fc8c65d8b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 — Avg Loss: 3920.1993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfb1f77da4146e4ae64821e0e2c7bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 — Avg Loss: 3860.9607\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric_temporal import ASTGCN\n",
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric_temporal import ASTGCN\n",
    "from torch_geometric.utils import dense_to_sparse  # for converting dense A → sparse edges\n",
    "# 1. Prepare device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Prepare the graph\n",
    "locations = {\n",
    "    \"Data_สถานีชาร์จ\": (13.73624, 100.52995),\n",
    "    \"Data_อาคารจามจุรี4\": (13.73260, 100.53177),\n",
    "    \"Data_อาคารจามจุรี 9\": (13.73380, 100.53045),\n",
    "    \"Data_อาคารจุลจักรพงษ์\": (13.73684, 100.52852),\n",
    "    \"Data_อาคารบรมราชกุมารี\": (13.73800, 100.52905),\n",
    "    \"Data_อาคารวิทยนิเวศน์\": (13.73723, 100.53015),\n",
    "}\n",
    "station_names = list(locations.keys())\n",
    "num_nodes = len(station_names)\n",
    "\n",
    "# Build a fully-connected edge_index\n",
    "edge_index = torch.tensor(\n",
    "    [[i, j] for i in range(num_nodes) for j in range(num_nodes) if i != j],\n",
    "    dtype=torch.long,\n",
    ").t().contiguous().to(device)  # ← move to GPU here\n",
    "\n",
    "# 3. Pivot utility\n",
    "def pivot_to_tensor(df, seq_len):\n",
    "    df_pivot = df.pivot(index='Date', columns='station_name', values='Electricity(kW)')\n",
    "    df_pivot = df_pivot[station_names].fillna(0.)\n",
    "    windows = []\n",
    "    for start in range(len(df_pivot) - seq_len + 1):\n",
    "        win = df_pivot.iloc[start:start+seq_len].values  # (seq_len, N)\n",
    "        windows.append(win.T)                            # (N, seq_len)\n",
    "    arr = np.stack(windows, axis=0)                     # (T, N, seq_len)\n",
    "    return torch.tensor(arr, dtype=torch.float)\n",
    "\n",
    "# 4. Data preparation\n",
    "len_input = 96\n",
    "prediction_length = 96\n",
    "X = pivot_to_tensor(train_df, len_input + prediction_length)\n",
    "X_input = X[:, :, :len_input]      # (T, N, len_input)\n",
    "X_target = X[:, :, len_input:]     # (T, N, prediction_length)\n",
    "\n",
    "class TemporalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_in, X_out):\n",
    "        self.X_in = X_in\n",
    "        self.X_out = X_out\n",
    "    def __len__(self):\n",
    "        return len(self.X_in)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_in[idx], self.X_out[idx]\n",
    "\n",
    "dataset = TemporalDataset(X_input, X_target)\n",
    "loader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "# Config dictionary for ASTGCN hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "class ASTGCN_V2(nn.Module):\n",
    "    def __init__(self, num_nodes: int, **kwargs):\n",
    "        super().__init__()\n",
    "        self.astgcn = ASTGCN(**kwargs)\n",
    "        # learnable factors to build A_adp = softmax(ReLU(emb1 @ emb2))\n",
    "        self.node_emb1 = nn.Parameter(torch.randn(num_nodes, 10))\n",
    "        self.node_emb2 = nn.Parameter(torch.randn(10, num_nodes))\n",
    "\n",
    "    def forward(self, x, edge_index=None):\n",
    "        # 1) build adaptive adjacency matrix (dense)\n",
    "        A_inter = F.relu(self.node_emb1 @ self.node_emb2)   # (N, N)\n",
    "        A_adp   = F.softmax(A_inter, dim=1)                 # row-normalize\n",
    "\n",
    "        # 2) convert to sparse edge_index (drop weights)\n",
    "        edge_index_adp, _ = dense_to_sparse(A_adp)          #  [oai_citation:0‡pytorch-geometric-temporal.readthedocs.io](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html?utm_source=chatgpt.com)\n",
    "\n",
    "        # 3) feed into ASTGCN (which expects only edge_index)  [oai_citation:1‡pytorch-geometric-temporal.readthedocs.io](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html?utm_source=chatgpt.com)\n",
    "        out = self.astgcn(x, edge_index_adp)\n",
    "\n",
    "        return F.relu(out)\n",
    "# 5. Instantiate and move model to GPU\n",
    "# Define your config as before\n",
    "astgcn_config = {\n",
    "    \"nb_block\": 2,\n",
    "    \"in_channels\": 1,\n",
    "    \"K\": 2,\n",
    "    \"nb_chev_filter\": 64,\n",
    "    \"nb_time_filter\": 64,\n",
    "    \"time_strides\": 1,\n",
    "    \"num_for_predict\": prediction_length,\n",
    "    \"len_input\": len_input,\n",
    "    \"num_of_vertices\": num_nodes,\n",
    "    \"normalization\": \"sym\",\n",
    "    \"bias\": True,\n",
    "}\n",
    "# 1) Model, loss, optimizer, scheduler, scaler\n",
    "model     = ASTGCN_V2(**astgcn_config, num_nodes=num_nodes).to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-2, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=3e-2,\n",
    "    steps_per_epoch=len(loader),\n",
    "    epochs=30,\n",
    "    pct_start=0.3   # optional: gentle warm-up\n",
    ")\n",
    "scaler    = GradScaler()\n",
    "\n",
    "# 2) Training loop\n",
    "model.train()\n",
    "for epoch in range(30):\n",
    "    total_loss = 0.0\n",
    "    # iterate over DataLoader directly\n",
    "    for X_batch, Y_batch in tqdm(loader, desc=f\"Epoch {epoch+1:02d}\"):\n",
    "        X = X_batch.unsqueeze(2).to(device)  # [B, N, 1, len_input]\n",
    "        Y = Y_batch.to(device)               # [B, N, pred_len]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            y_pred = model(X, edge_index)\n",
    "            loss   = criterion(y_pred, Y)\n",
    "\n",
    "        # 1) backward with scaler\n",
    "        scaler.scale(loss).backward()\n",
    "        # 2) un-scale then clip\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # 3) optimizer step + scaler update\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # 4) scheduler step (per-batch)\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1:02d} — Avg Loss: {avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class ASTGCNWrapper(nn.Module):\n",
    "#     def __init__(self, model, edge_index):\n",
    "#         super().__init__()\n",
    "#         self.model = model\n",
    "#         self.edge_index = edge_index\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x, self.edge_index)\n",
    "# from torchview import draw_graph\n",
    "\n",
    "# # Wrap the model with fixed edge_index\n",
    "# wrapped_model = ASTGCNWrapper(model, edge_index)\n",
    "\n",
    "# # Provide the correct input shape: (batch_size, num_nodes, 1, len_input)\n",
    "# draw_graph(\n",
    "#     wrapped_model,\n",
    "#     input_size=(1, num_nodes, 1, len_input),\n",
    "#     expand_nested=True,\n",
    "#     roll=True,\n",
    "#     show_shapes=True,\n",
    "# ).visual_graph.render(\"astgcn_graph_v1.5\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T16:54:19.279495Z",
     "iopub.status.busy": "2025-07-15T16:54:19.279217Z",
     "iopub.status.idle": "2025-07-15T16:54:45.684702Z",
     "shell.execute_reply": "2025-07-15T16:54:45.683923Z",
     "shell.execute_reply.started": "2025-07-15T16:54:19.279478Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0530a2a817444e70ba7da86095942114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batch:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE (first‐step, dropping 570 rows with no pred): 58.8885\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Pivot and concatenate train+test\n",
    "df_all = pd.concat([train_df, test_df], ignore_index=True)\n",
    "pivot = (df_all\n",
    "         .pivot(index='Date', columns='station_name', values='Electricity(kW)')\n",
    "         .reindex(columns=station_names)      # ensure correct station order\n",
    "         .fillna(0.0))\n",
    "dates = pivot.index\n",
    "T = len(dates)\n",
    "\n",
    "# 2. Build every possible sliding window of length `len_input`\n",
    "max_start = T - len_input - prediction_length + 1  # total windows\n",
    "windows = []\n",
    "for t0 in range(max_start):\n",
    "    arr = pivot.iloc[t0:t0+len_input].values       # (len_input, N)\n",
    "    windows.append(arr.T)                          # → (N, len_input)\n",
    "X_all = np.stack(windows, axis=0)                  # (W, N, len_input)\n",
    "X_all = torch.from_numpy(X_all).float().unsqueeze(2)  # (W, N, 1, len_input)\n",
    "\n",
    "# 3. Batch through the model in eval mode\n",
    "batch_size = 512\n",
    "loader = DataLoader(TensorDataset(X_all), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for (Xb,) in tqdm(loader,desc=\"batch\"):\n",
    "        Xb = Xb.to(device)\n",
    "        yb = model(Xb, edge_index)               # → (B, N, prediction_length)\n",
    "        preds.append(yb.cpu().numpy())\n",
    "preds = np.concatenate(preds, axis=0)            # (W, N, pred_len)\n",
    "\n",
    "# 4. Take only the *first-step* forecast (you can slice other horizons similarly)\n",
    "first_step = preds[:, :, 0]                      # (W, N)\n",
    "\n",
    "# 5. Build a long DataFrame of all predictions\n",
    "#    window w predicts for date = dates[w + len_input]\n",
    "pred_dates = dates[len_input : len_input + first_step.shape[0]]\n",
    "records = []\n",
    "for w, pd_dt in enumerate(pred_dates):\n",
    "    for i, station in enumerate(station_names):\n",
    "        records.append((pd_dt, station, first_step[w, i]))\n",
    "df_preds = pd.DataFrame(records, columns=['Date','station_name','Predicted(kW)'])\n",
    "\n",
    "# 6. Merge with test_df (this yields exactly len(test_df)=40 839 rows)\n",
    "df_merged = (test_df\n",
    "             .merge(df_preds, on=['Date','station_name'], how='left')\n",
    "             .sort_values(['Date','station_name'])\n",
    "             .reset_index(drop=True))\n",
    "\n",
    "# after your merge:\n",
    "df_eval = df_merged.dropna(subset=['Predicted(kW)']).copy()\n",
    "\n",
    "# compute MAE only on the non‐NaN rows\n",
    "mae = mean_absolute_error(\n",
    "    df_eval['Electricity(kW)'].values,\n",
    "    df_eval['Predicted(kW)'].values\n",
    ")\n",
    "print(f\"Test MAE (first‐step, dropping {len(df_merged) - len(df_eval)} rows with no pred): {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T16:54:49.467515Z",
     "iopub.status.busy": "2025-07-15T16:54:49.466491Z",
     "iopub.status.idle": "2025-07-15T16:54:49.483435Z",
     "shell.execute_reply": "2025-07-15T16:54:49.482667Z",
     "shell.execute_reply.started": "2025-07-15T16:54:49.467489Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAPE (weighted): 0.6406 or 64.06%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Merge weights into evaluation DataFrame\n",
    "df_eval = df_eval.merge(station_weights_df, on='station_name', how='left')\n",
    "\n",
    "# Compute weighted absolute error\n",
    "df_eval['abs_error'] = np.abs(df_eval['Electricity(kW)'] - df_eval['Predicted(kW)'])\n",
    "df_eval['weighted_abs_error'] = df_eval['abs_error'] * df_eval['normalized_reverse_weight']\n",
    "\n",
    "# Compute weighted actual value\n",
    "df_eval['weighted_actual'] = df_eval['Electricity(kW)'] * df_eval['normalized_reverse_weight']\n",
    "\n",
    "# Calculate WAPE\n",
    "wape = df_eval['weighted_abs_error'].sum() / df_eval['weighted_actual'].sum()\n",
    "print(f\"WAPE (weighted): {wape:.4f} or {wape*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_eval.to_csv(\"df_eval.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7869354,
     "sourceId": 12473148,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
